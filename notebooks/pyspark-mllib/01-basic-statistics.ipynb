{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "\n",
    "The contents in this notebook are referenced from https://spark.apache.org/docs/latest/ml-statistics.html.\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Calculating the correlation between two series of data is a common operation in Statistics. In spark.ml we provide the flexibility to calculate pairwise correlations among many series. The supported correlation methods are currently Pearson’s and Spearman’s correlation.\n",
    "\n",
    "Correlation computes the correlation matrix for the input Dataset of Vectors using the specified method. The output will be a DataFrame that contains the correlation matrix of the column of vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/06 21:31:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"CorrelationExample\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [\n",
    "    (Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "    (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "    (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "    (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)\n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data, ['features'])\n",
    "\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/06 21:33:52 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/01/06 21:33:52 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/01/06 21:33:52 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n",
      "/Users/winsonyeap/miniconda3/envs/sandbox/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/06 21:33:53 WARN PearsonCorrelation: Pearson correlation matrix contains NaN values.\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(data_df, 'features').head()\n",
    "print(f\"Pearson correlation matrix:\\n{r1[0]}\")\n",
    "\n",
    "r2 = Correlation.corr(data_df, 'features', 'spearman').head()\n",
    "print(f\"Spearman correlation matrix:\\n{r2[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Hypothesis testing is a powerful tool in statistics to determine whether a result is statistically significant, whether this result occurred by chance or not. spark.ml currently supports Pearson’s Chi-squared ( χ2) tests for independence.\n",
    "\n",
    "### ChiSquareTest\n",
    "\n",
    "ChiSquareTest conducts Pearson’s independence test for every feature against the label. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the Chi-squared statistic is computed. All label and feature values must be categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|  features|\n",
      "+-----+----------+\n",
      "|  0.0|[0.5,10.0]|\n",
      "|  0.0|[1.5,20.0]|\n",
      "|  1.0|[1.5,30.0]|\n",
      "|  0.0|[3.5,30.0]|\n",
      "|  0.0|[3.5,40.0]|\n",
      "|  1.0|[3.5,40.0]|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "data_df = spark.createDataFrame(data, ['label', 'features'])\n",
    "\n",
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/winsonyeap/miniconda3/envs/sandbox/lib/python3.8/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degressOfFreedom: [2, 3]\n",
      "statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "r = ChiSquareTest.test(data_df, 'features', 'label').head()\n",
    "print(f\"pValues: {r.pValues}\")\n",
    "print(f\"degressOfFreedom: {r.degreesOfFreedom}\")\n",
    "print(f\"statistics: {r.statistics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizer\n",
    "\n",
    "We provide vector column summary statistics for Dataframe through Summarizer. Available metrics are the column-wise max, min, mean, sum, variance, std, and number of nonzeros, as well as the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "|weight|     features|\n",
      "+------+-------------+\n",
      "|   1.0|[1.0,1.0,1.0]|\n",
      "|   0.0|[1.0,2.0,3.0]|\n",
      "+------+-------------+\n",
      "\n",
      "+-----------------------------------+\n",
      "|aggregate_metrics(features, weight)|\n",
      "+-----------------------------------+\n",
      "|{[1.0,1.0,1.0], 1}                 |\n",
      "+-----------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|aggregate_metrics(features, 1.0)|\n",
      "+--------------------------------+\n",
      "|{[1.0,1.5,2.0], 2}              |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.0,1.0] |\n",
      "+--------------+\n",
      "\n",
      "+--------------+\n",
      "|mean(features)|\n",
      "+--------------+\n",
      "|[1.0,1.5,2.0] |\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# Getting the SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "data_df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                          Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "data_df.show()\n",
    "\n",
    "# Create summarizer for multiple metrics \"mean\" and \"count\"\n",
    "summarizer = Summarizer.metrics('mean', 'count')\n",
    "\n",
    "# Compute statistics for multiple metrics with weight\n",
    "data_df.select(summarizer.summary(data_df.features, data_df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for multiple metrics without weight\n",
    "data_df.select(summarizer.summary(data_df.features)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metric 'mean' with weight\n",
    "data_df.select(Summarizer.mean(data_df.features, data_df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metric 'mean' without weight\n",
    "data_df.select(Summarizer.mean(data_df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Close all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c6f73c0f8ce983d4b9e5351dc51a6e0fdcbe27e9fb3ff4486bf93637926e330f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('sandbox': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
